{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of this project is to develop a **classification model to predict the positive/negative labels** of movie reviews. This prediction will be **based solely on the text content** of the reviews.\n",
    "\n",
    "The data used in this project is the polarity dataset v2.0, http://www.cs.cornell.edu/people/pabo/movie-review-data/, of Cornell University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/moviereviews.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0   neg  how do films like mouse hunt get into theatres...\n",
       "1   neg  some talented actresses are blessed with a dem...\n",
       "2   pos  this has been an extraordinary year for austra...\n",
       "3   pos  according to hollywood movies made in last few...\n",
       "4   neg  my first press screening of 1998 and already i..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    1000\n",
       "pos    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of both labels\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how do films like mouse hunt get into theatres ? \r\n",
      "isn't there a law or something ? \r\n",
      "this diabolical load of claptrap from steven speilberg's dreamworks studio is hollywood family fare at its deadly worst . \r\n",
      "mouse hunt takes the bare threads of a plot and tries to prop it up with overacting and flat-out stupid slapstick that makes comedies like jingle all the way look decent by comparison . \r\n",
      "writer adam rifkin and director gore verbinski are the names chiefly responsible for this swill . \r\n",
      "the plot , for what its worth , concerns two brothers ( nathan lane and an appalling lee evens ) who inherit a poorly run string factory and a seemingly worthless house from their eccentric father . \r\n",
      "deciding to check out the long-abandoned house , they soon learn that it's worth a fortune and set about selling it in auction to the highest bidder . \r\n",
      "but battling them at every turn is a very smart mouse , happy with his run-down little abode and wanting it to stay that way . \r\n",
      "the story alternates between unfunny scenes of the brothers bickering over what to do with their inheritance and endless action sequences as the two take on their increasingly determined furry foe . \r\n",
      "whatever promise the film starts with soon deteriorates into boring dialogue , terrible overacting , and increasingly uninspired slapstick that becomes all sound and fury , signifying nothing . \r\n",
      "the script becomes so unspeakably bad that the best line poor lee evens can utter after another run in with the rodent is : \" i hate that mouse \" . \r\n",
      "oh cringe ! \r\n",
      "this is home alone all over again , and ten times worse . \r\n",
      "one touching scene early on is worth mentioning . \r\n",
      "we follow the mouse through a maze of walls and pipes until he arrives at his makeshift abode somewhere in a wall . \r\n",
      "he jumps into a tiny bed , pulls up a makeshift sheet and snuggles up to sleep , seemingly happy and just wanting to be left alone . \r\n",
      "it's a magical little moment in an otherwise soulless film . \r\n",
      "a message to speilberg : if you want dreamworks to be associated with some kind of artistic credibility , then either give all concerned in mouse hunt a swift kick up the arse or hire yourself some decent writers and directors . \r\n",
      "this kind of rubbish will just not do at all . \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check first negative review\n",
    "\n",
    "print(df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this has been an extraordinary year for australian films . \r\n",
      " \" shine \" has just scooped the pool at the australian film institute awards , picking up best film , best actor , best director etc . to that we can add the gritty \" life \" ( the anguish , courage and friendship of a group of male prisoners in the hiv-positive section of a jail ) and \" love and other catastrophes \" ( a low budget gem about straight and gay love on and near a university campus ) . \r\n",
      "i can't recall a year in which such a rich and varied celluloid library was unleashed from australia . \r\n",
      " \" shine \" was one bookend . \r\n",
      "stand by for the other one : \" dead heart \" . \r\n",
      ">from the opening credits the theme of division is established . \r\n",
      "the cast credits have clear and distinct lines separating their first and last names . \r\n",
      "bryan | brown . \r\n",
      "in a desert settlement , hundreds of kilometres from the nearest town , there is an uneasy calm between the local aboriginals and the handful of white settlers who live nearby . \r\n",
      "the local police officer has the task of enforcing \" white man's justice \" to the aboriginals . \r\n",
      "these are people with a proud 40 , 000 year heritage behind them . \r\n",
      "naturally , this includes their own system of justice ; key to which is \" payback \" . \r\n",
      "an eye for an eye . \r\n",
      "revenge . \r\n",
      "usually extracted by the spearing through of the recipient's thigh . \r\n",
      "brown , as the officer , manages quite well to keep the balance . \r\n",
      "he admits that he has to 'bend the rules' a bit , including actively encouraging at least one brutal \" payback \" . \r\n",
      " ( be warned that this scene , near the start , is not for the squeamish ) . \r\n",
      "the local priest - an aboriginal , but in the \" white fellas \" church - has a foot on either side of the line . \r\n",
      "he is , figuratively and literally , in both camps . \r\n",
      "ernie dingo brings a great deal of understanding to this role as the man in the middle . \r\n",
      "he is part churchman and part politician . \r\n",
      "however the tension , like the heat , flies and dust , is always there . \r\n",
      "whilst her husband - the local teacher - is in church , white lady kate ( milliken ) and her aborginal friend tony , ( pedersen ) have gone off into the hills . \r\n",
      "he takes her to a sacred site , even today strictly men-only . \r\n",
      "she appears to not know this . \r\n",
      "tony tells her that this is a special place , an initiation place . \r\n",
      "he then makes love to her , surrounded by ancient rock art . \r\n",
      "the community finds out about this sacrilegious act and it's payback time . \r\n",
      "the fuse is lit and the brittle inter-racial peace is shattered . \r\n",
      "everyone is affected in the fall out . \r\n",
      "to say more is to give away the details of this finely crafted film . \r\n",
      "suffice to say it's a rewarding experience . \r\n",
      "bryan brown , acting and co-producing , is the pivotal character . \r\n",
      "his officer is real , human and therefore flawed . \r\n",
      "brown comments that he expects audiences to feel warmth towards the man , then suddenly feel angry about him . \r\n",
      "it wasn't long ago that i visited central australia - ayers rock ( uluru ) and alice springs - for the first time . \r\n",
      "the wide-screen cinematography shows the dead heart of australia in a way that captures it's vicious beauty , but never deteriorates into a moving slide show , in which the gorgeous background dominates those pesky actors in the foreground . \r\n",
      "the cultural clash has provided the thesis for many a film ; from the western to the birdcage . \r\n",
      "at least three excellent australian films have covered the aboriginal people and the line between them and we anglo-saxon 'invaders' : \" jedda \" , \" the chant of jimmie blacksmith \" and \" the last wave \" . \r\n",
      "in a year when the race 'debate' has reared up in australia , it is nourishing to see such an intelligent , non-judgemental film as \" dead heart \" . \r\n",
      "the aboriginal priest best sums this up . \r\n",
      "he is asked to say if he is a \" black fella or white fella \" . \r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check first positive review\n",
    "\n",
    "print(df['review'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "review    35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 missing reviews. We should delete these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing reviews\n",
    "\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     0\n",
       "review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the isspace() method\n",
    "\n",
    "empty_strings = []\n",
    "\n",
    "for i, lb, rv in df.itertuples():\n",
    "    if rv.isspace():\n",
    "        empty_strings.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 71, 147, 151, 283, 307, 313, 323, 343, 351, 427, 501, 633, 675, 815, 851, 977, 1079, 1299, 1455, 1493, 1525, 1531, 1763, 1851, 1905, 1993]\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(empty_strings)\n",
    "print(len(empty_strings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 27 reviews that correspond to empty strings. These reviews are identified by the indices in the empty_strings list. We should remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty strings\n",
    "\n",
    "df.drop(empty_strings, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg    969\n",
       "pos    969\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of both labels\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1938 movie reviews (969 are positive and 969 are negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Build pipeline to vectorize the data and train/fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                    ('clf', LinearSVC())])\n",
    "\n",
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Make predictions with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[235  47]\n",
      " [ 41 259]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.83      0.84       282\n",
      "         pos       0.85      0.86      0.85       300\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       582\n",
      "   macro avg       0.85      0.85      0.85       582\n",
      "weighted avg       0.85      0.85      0.85       582\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8487972508591065\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based solely on the text content of the reviews we've managed to correctly classify **84,9%** of them as positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Test the fitted model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos' 'neg']\n"
     ]
    }
   ],
   "source": [
    "# make up some reviews and test the model\n",
    "\n",
    "my_review = \"The movie was great! The main actors were superb and the storyline was convincing.\"\n",
    "my_review_2 = \"Terrible movie! A complete waste of money.\"\n",
    "\n",
    "reviews = [my_review, my_review_2]\n",
    "\n",
    "print(text_clf.predict(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a very simple model, everything seems to be working just fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used **sklearn's TfidfVectorizer** - our text vectorization method - and the **Linear Support Vector Classification algorithm** to build our model. \n",
    "\n",
    "Let's consider other alternatives in terms of **text normalization and vectorization methods** and learning **algorithms**, that can be useful in different **text classification** scenarios.\n",
    "\n",
    "In step 2 we'll be focusing on the different ways how we can **normalize** our documents (reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization allows us to **reduce our vocabulary size** and hence the number of dimensions of our feature space, which results in efficiency gains in terms of storage and processing.\n",
    "\n",
    "However, text normalization comes with the cost of **loss of information**.\n",
    "\n",
    "It is up to us to find the right balance between those efficiency gains and our model's accuracy, considering both recall and precision.\n",
    "\n",
    "Our ultimate goal is to **improve the overall performance of our NLP pipeline**, given the specific objectives of our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the text vectorization method we applied in step 1 deals with text normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check parameters of TfidfVectorizer  used in our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.float64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'norm': 'l2',\n",
       " 'preprocessor': None,\n",
       " 'smooth_idf': True,\n",
       " 'stop_words': None,\n",
       " 'strip_accents': None,\n",
       " 'sublinear_tf': False,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'use_idf': True,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.named_steps['tfidf'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding text normalization and having a look at the default parameters that were used before, we can see that:\n",
    "* **tokens of 2 or more alphanumeric characters** were selected (**punctuation was ignored**)\n",
    "* tokens were **lowercased**\n",
    "\n",
    "And that:\n",
    "* **no stopwords** were removed\n",
    "* apart from that, **no stemmer and no lemmatizer** were used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check dataset size and vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1938"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33855"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_clf.named_steps['tfidf'].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a reasonably small dataset and vocabulary, and thus we can choose to keep most of the information from our reviews without hindering our NLP pipeline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposing this was not the case, what could we do? In order to maximize the range of possibilities, we could make some changes to our pipeline by creating **custom transformers for both text normalization and text vectorization** and **building some different models** using several distinct algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a **custom text normalization transformer** that allows us to:\n",
    "* lowercase words\n",
    "* remove punctuation\n",
    "* remove stopwords\n",
    "* stem words\n",
    "* lemmatize words\n",
    "* lemmatize and stem words (in this order)\n",
    "\n",
    "This is our step 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perform necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "from nltk import pos_tag, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a custom text normalization transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english', lemmatizer='wordnet', stemmer=None, vectorizer='gensim'):\n",
    "        '''\n",
    "        The default language is English.\n",
    "        The default lemmatizer is WordNetLemmatizer. If you don't want to use a lemmatizer,\n",
    "        set it to None.\n",
    "        To use a stemmer, set stemmer to 'porter' (PorterStemmer) or to 'snowball' (SnowballStemmer),\n",
    "        otherwise, it defaults to None.\n",
    "        The default vectorizer is Gensim - this way you get a list of tokens as an output.\n",
    "        If you need a string instead, set vectorizer to 'other'.\n",
    "        '''\n",
    "        self.language = language\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(self.language))\n",
    "        \n",
    "        if lemmatizer == 'wordnet':\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        else:\n",
    "            self.lemmatizer = None\n",
    "        \n",
    "        if stemmer == 'porter':\n",
    "            self.stemmer = PorterStemmer()\n",
    "        elif stemmer == 'snowball':\n",
    "            self.stemmer = SnowballStemmer(self.language)\n",
    "        else:\n",
    "            self.stemmer = None\n",
    "        \n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "    def is_punct(self, token):\n",
    "        # returns True if all characters of a token are punctuation signs\n",
    "        return all(unicodedata.category(char).startswith('P') for char in token)\n",
    "    \n",
    "    def is_stopword(self, token):\n",
    "        # returns True if the lowercased token is a stopword\n",
    "        return token.lower() in self.stopwords\n",
    "    \n",
    "    def lemmatize(self, token, tag):\n",
    "        '''\n",
    "        Converts Penn Treebank part-of-speech tags (the default tag set in nltk.pos_tag)\n",
    "        to WordNet tags - defaults to wn.Noun if the first letter of the Penn Treebank pos tag\n",
    "        is neither 'N', 'V', 'R' or 'J'.\n",
    "        Returns lemmatized token.\n",
    "        '''\n",
    "        wordnet_tag = {\n",
    "            'N': wn.NOUN, \n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        \n",
    "        return self.lemmatizer.lemmatize(token, wordnet_tag)\n",
    "    \n",
    "    def stem(self, token):\n",
    "        return self.stemmer.stem(token)\n",
    "    \n",
    "    def normalize(self, review):\n",
    "        '''\n",
    "        Normalizes review by removing punctuation and stopwords, lowercasing tokens,\n",
    "        and by lemmatizing or/and stemming tokens.\n",
    "        '''\n",
    "        if not self.lemmatizer == None and self.stemmer == None:\n",
    "            return [self.lemmatize(token, tag).lower() \n",
    "                    for sentence in sent_tokenize(review) \n",
    "                    for (token, tag) in pos_tag(wordpunct_tokenize(sentence)) \n",
    "                    if not self.is_punct(token) and not self.is_stopword(token)]\n",
    "\n",
    "        elif self.lemmatizer == None and not self.stemmer == None:\n",
    "            return [self.stem(token).lower() \n",
    "                    for sentence in sent_tokenize(review) \n",
    "                    for (token, tag) in pos_tag(wordpunct_tokenize(sentence)) \n",
    "                    if not self.is_punct(token) and not self.is_stopword(token)]\n",
    "\n",
    "        else:\n",
    "            return [self.stem(lemmatized_token)\n",
    "                    for lemmatized_token in [self.lemmatize(token, tag).lower() \n",
    "                    for sentence in sent_tokenize(review) \n",
    "                    for (token, tag) in pos_tag(wordpunct_tokenize(sentence)) \n",
    "                    if not self.is_punct(token) and not self.is_stopword(token)]]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, reviews):\n",
    "        # returns a list of tokens if vectorizer is set to 'gensim', otherwise, returns a string\n",
    "        for review in reviews:\n",
    "            if self.vectorizer == 'gensim':\n",
    "                yield self.normalize(review)\n",
    "            else:\n",
    "                yield ' '.join(self.normalize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Test the custom text normalization transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate object with default values\n",
    "normalizer = TextNormalizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our class, TextNormalizer, inherits from BaseEstimator. Its objects are then estimators with **methods get_params() and set_params()**.\n",
    "\n",
    "Let's check our object's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'english',\n",
       " 'lemmatizer': <WordNetLemmatizer>,\n",
       " 'stemmer': None,\n",
       " 'vectorizer': 'gensim'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language is **English**, the lemmatizer we'll be using is **WorNetLemmatizer**, **no stemming** will be performed, and the output will be a **list of tokens**, ready to use as input to a Gensim's text vectorization method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a side note, both stemming and lemmatization allow us to reduce the size of our vocabulary, but they accomplish it in different ways:\n",
    "* **stemming** uses a series of rules to remove word affixes and the resulting token **might not be a valid word**\n",
    "* **lemmatization** uses a knowledge base and may also use the word's POS tag to return the word's lemma, which **is always a valid word**\n",
    "\n",
    "This is way **it makes sense to use a stemmer right after a lemmatizer, but not the other way around**, if you goal is to reduce dimensionality and improve recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextNormalizer also inherits from TransformerMixin, so it has a **fit_transform method**.\n",
    "\n",
    "Since our transform method yields a generator object, we use list() to consume the generator. Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_normalized'] = list(normalizer.fit_transform(df['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "      <th>review_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>how do films like mouse hunt get into theatres...</td>\n",
       "      <td>[film, like, mouse, hunt, get, theatre, law, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>some talented actresses are blessed with a dem...</td>\n",
       "      <td>[talented, actress, bless, demonstrated, wide,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>this has been an extraordinary year for austra...</td>\n",
       "      <td>[extraordinary, year, australian, film, shine,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos</td>\n",
       "      <td>according to hollywood movies made in last few...</td>\n",
       "      <td>[accord, hollywood, movie, make, last, decade,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>my first press screening of 1998 and already i...</td>\n",
       "      <td>[first, press, screening, 1998, already, get, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review  \\\n",
       "0   neg  how do films like mouse hunt get into theatres...   \n",
       "1   neg  some talented actresses are blessed with a dem...   \n",
       "2   pos  this has been an extraordinary year for austra...   \n",
       "3   pos  according to hollywood movies made in last few...   \n",
       "4   neg  my first press screening of 1998 and already i...   \n",
       "\n",
       "                                   review_normalized  \n",
       "0  [film, like, mouse, hunt, get, theatre, law, s...  \n",
       "1  [talented, actress, bless, demonstrated, wide,...  \n",
       "2  [extraordinary, year, australian, film, shine,...  \n",
       "3  [accord, hollywood, movie, make, last, decade,...  \n",
       "4  [first, press, screening, 1998, already, get, ...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be working as expected. Our next step it to create a **custom Gensim vectorization transformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
